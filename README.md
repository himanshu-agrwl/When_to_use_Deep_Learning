# When_to_use_Deep_Learning
This repository explores model selection across data types by comparing Logistic Regression, ANN, and CNN. It demonstrates why classical ML and ANNs work well for tabular data, while CNNs are necessary for image-based tasks, using breast cancer data and MNIST as case studies.

Overview
This repository presents a comparative study on model selection in machine learning, focusing on when classical models and ANNs are sufficient and when CNNs become essential.

Rather than assuming deep learning is always superior, this project starts with strong baseline models, evaluates their performance, and then justifies the transition to more complex architectures based on data characteristics.

ğŸ¯ Objectives

Establish Logistic Regression as a baseline for tabular data

Evaluate whether ANN provides meaningful improvement over classical ML

Demonstrate why CNNs are necessary for image-based tasks

Perform quantitative and qualitative evaluation, not just accuracy comparison

ğŸ—‚ï¸ Project Structure
ann-to-cnn-model-selection/
â”‚
â”œâ”€â”€ tabular-data/
â”‚   â”œâ”€â”€ logistic_regression.ipynb
â”‚   â”œâ”€â”€ ann_tabular.ipynb
â”‚   â””â”€â”€ results.md
â”‚
â”œâ”€â”€ image-data/
â”‚   â”œâ”€â”€ cnn_mnist.ipynb
â”‚   â””â”€â”€ results.md
â”‚
â”œâ”€â”€ README.md

ğŸ“Š Part 1: Tabular Data â€” Logistic Regression & ANN
Dataset

Breast Cancer Wisconsin Dataset

30 numerical features

Binary classification task

ğŸ”¹ Baseline Model: Logistic Regression

Logistic Regression is used as the baseline model because:

It performs strongly on linearly separable tabular data

It provides interpretability

It sets a realistic performance benchmark

ğŸ”¹ ANN Model

A fully connected Artificial Neural Network (ANN) was trained on the same dataset to evaluate whether a deeper model adds value.

âœ… Key Observation

ANN and Logistic Regression achieved similar accuracy and confusion matrices

Error patterns were nearly identical

ğŸ§  Insight

On structured tabular data with well-engineered features, increasing model complexity does not necessarily improve performance.

This validates the importance of starting with baselines before applying deep learning.

ğŸ–¼ï¸ Part 2: Image Data â€” CNN from Scratch
Dataset

MNIST handwritten digits

28Ã—28 grayscale images

10-class classification problem

ğŸ”¹ Why CNN?

Unlike tabular data, image data has:

Spatial structure

Local pixel dependencies

Translation invariance requirements

ANNs and classical ML models fail to exploit these properties effectively.

ğŸ”¹ CNN Architecture

Convolutional layers for feature extraction

MaxPooling for spatial reduction

Dense layers for classification

Softmax output for multi-class prediction

The CNN was built from scratch, without transfer learning.

ğŸ“ˆ Evaluation & Analysis
Metrics Used

Accuracy

Confusion Matrix

Prediction Visualization

Error (misclassification) Analysis

ğŸ”¹ Confusion Matrix Heatmap

Highlights systematic confusions between visually similar digits

Shows errors are due to data ambiguity, not random failure

ğŸ”¹ Prediction Visualizations

Sample correct predictions for interpretability

Visualization of incorrect predictions for error analysis

Automatic comparison with ground truth labels (no manual inspection)

ğŸ§  Key Learnings

Deep learning is not always necessary

Baselines matter and should not be skipped

ANN â‰  CNN â€” architecture must match data structure

CNNs excel when spatial information is critical

Model evaluation should include qualitative analysis, not just metrics

ğŸš€ Technologies Used

Python

NumPy

Scikit-learn

TensorFlow / Keras

Matplotlib & Seaborn

Google Colab

ğŸ“Œ Final Takeaway

Model selection should be driven by data characteristics, not by model complexity.
This project demonstrates a principled transition from Logistic Regression â†’ ANN â†’ CNN based on empirical evidence.

ğŸ”— Future Work

Apply CNNs to more complex image datasets (CIFAR-10)

Experiment with data augmentation

Compare CNN vs ANN on flattened image inputs

Explore transfer learning
